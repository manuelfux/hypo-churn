{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Advanced Model Training - XGBoost & Hyperparameter Tuning\n",
    "\n",
    "Dieses Notebook fokussiert sich auf:\n",
    "1. **XGBoost** - Gradient Boosting Classifier\n",
    "2. **Class Imbalance** - SMOTE und Class Weights\n",
    "3. **Hyperparameter Tuning** - GridSearch für optimale Performance\n",
    "4. **Model Comparison** - Vergleich mit Baseline Models\n",
    "\n",
    "**Problem aus Baseline:** Recall nur 45% → Viele Churner werden nicht erkannt\n",
    "\n",
    "**Ziel:** Recall auf 60-70% verbessern bei guter Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Alle Imports erfolgreich\n",
      "XGBoost Version: 2.1.4\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, confusion_matrix, classification_report, roc_curve\n",
    ")\n",
    "\n",
    "# XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# SMOTE for imbalanced data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Project modules\n",
    "import sys\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
    "from hypo_churn.evaluation import evaluate_model, print_evaluation_results\n",
    "\n",
    "# Plotting\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ Alle Imports erfolgreich\")\n",
    "print(f\"XGBoost Version: {__import__('xgboost').__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_data_header",
   "metadata": {},
   "source": [
    "## 1. Daten laden & vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Daten geladen: (10000, 20)\n",
      "Churn Rate: 20.37%\n"
     ]
    }
   ],
   "source": [
    "# Pfade\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "DATA_DIR = PROJECT_ROOT / 'data' / 'processed'\n",
    "MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Daten laden\n",
    "df = pd.read_csv(DATA_DIR / 'banking_churn_mortgage_adapted.csv')\n",
    "print(f\"✓ Daten geladen: {df.shape}\")\n",
    "print(f\"Churn Rate: {df['churned'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "prepare_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 17\n",
      "Samples: 10000\n",
      "\n",
      "Train: 8000 | Test: 2000\n",
      "Train Churn Rate: 20.38%\n",
      "Test Churn Rate: 20.35%\n"
     ]
    }
   ],
   "source": [
    "# Feature Selection\n",
    "drop_columns = ['RowNumber', 'CustomerId', 'Surname', 'churned']\n",
    "feature_cols = [col for col in df.columns if col not in drop_columns]\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df['churned']\n",
    "\n",
    "# Encoding\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "if categorical_features:\n",
    "    X = pd.get_dummies(X, columns=categorical_features, drop_first=True)\n",
    "\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Samples: {X.shape[0]}\")\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain: {X_train.shape[0]} | Test: {X_test.shape[0]}\")\n",
    "print(f\"Train Churn Rate: {y_train.mean():.2%}\")\n",
    "print(f\"Test Churn Rate: {y_test.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baseline_header",
   "metadata": {},
   "source": [
    "## 2. Baseline: XGBoost (Default Parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "xgb_baseline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BASELINE: XGBoost (Default Parameters)\n",
      "======================================================================\n",
      "\n",
      "==================================================\n",
      "Model Evaluation Results\n",
      "==================================================\n",
      "Accuracy: 0.8505\n",
      "Precision: 0.6957\n",
      "Recall: 0.4717\n",
      "F1 Score: 0.5622\n",
      "Roc Auc: 0.8382\n",
      "==================================================\n",
      "\n",
      "Training Time: 1.15s\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"BASELINE: XGBoost (Default Parameters)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# XGBoost mit Default Parameters\n",
    "xgb_baseline = XGBClassifier(\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "xgb_baseline.fit(X_train, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "# Predictions\n",
    "y_pred_baseline = xgb_baseline.predict(X_test)\n",
    "y_proba_baseline = xgb_baseline.predict_proba(X_test)\n",
    "\n",
    "# Evaluation\n",
    "metrics_baseline = evaluate_model(y_test, y_pred_baseline, y_proba_baseline)\n",
    "print_evaluation_results(metrics_baseline)\n",
    "print(f\"Training Time: {train_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smote_header",
   "metadata": {},
   "source": [
    "## 3. XGBoost mit SMOTE (Class Imbalance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "xgb_smote",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "XGBoost + SMOTE (Oversampling)\n",
      "======================================================================\n",
      "Original Training Set: 8000 samples\n",
      "After SMOTE: 12740 samples\n",
      "Original Churn Rate: 20.38%\n",
      "After SMOTE Churn Rate: 50.00%\n",
      "\n",
      "==================================================\n",
      "Model Evaluation Results\n",
      "==================================================\n",
      "Accuracy: 0.8300\n",
      "Precision: 0.5753\n",
      "Recall: 0.6290\n",
      "F1 Score: 0.6009\n",
      "Roc Auc: 0.8344\n",
      "==================================================\n",
      "\n",
      "Training Time: 1.34s\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"XGBoost + SMOTE (Oversampling)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# SMOTE anwenden\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"Original Training Set: {X_train.shape[0]} samples\")\n",
    "print(f\"After SMOTE: {X_train_smote.shape[0]} samples\")\n",
    "print(f\"Original Churn Rate: {y_train.mean():.2%}\")\n",
    "print(f\"After SMOTE Churn Rate: {y_train_smote.mean():.2%}\")\n",
    "\n",
    "# XGBoost mit SMOTE-Daten\n",
    "xgb_smote = XGBClassifier(\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "xgb_smote.fit(X_train_smote, y_train_smote)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "# Predictions (auf originalen Test-Daten)\n",
    "y_pred_smote = xgb_smote.predict(X_test)\n",
    "y_proba_smote = xgb_smote.predict_proba(X_test)\n",
    "\n",
    "# Evaluation\n",
    "metrics_smote = evaluate_model(y_test, y_pred_smote, y_proba_smote)\n",
    "print_evaluation_results(metrics_smote)\n",
    "print(f\"Training Time: {train_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "class_weight_header",
   "metadata": {},
   "source": [
    "## 4. XGBoost mit Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "xgb_class_weight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "XGBoost + Class Weights\n",
      "======================================================================\n",
      "Scale Pos Weight: 3.91\n",
      "\n",
      "==================================================\n",
      "Model Evaluation Results\n",
      "==================================================\n",
      "Accuracy: 0.8340\n",
      "Precision: 0.5874\n",
      "Recall: 0.6192\n",
      "F1 Score: 0.6029\n",
      "Roc Auc: 0.8386\n",
      "==================================================\n",
      "\n",
      "Training Time: 1.17s\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"XGBoost + Class Weights\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Scale_pos_weight berechnen\n",
    "# Formel: (anzahl_negative) / (anzahl_positive)\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "print(f\"Scale Pos Weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "# XGBoost mit Class Weight\n",
    "xgb_weighted = XGBClassifier(\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False,\n",
    "    scale_pos_weight=scale_pos_weight\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "xgb_weighted.fit(X_train, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "# Predictions\n",
    "y_pred_weighted = xgb_weighted.predict(X_test)\n",
    "y_proba_weighted = xgb_weighted.predict_proba(X_test)\n",
    "\n",
    "# Evaluation\n",
    "metrics_weighted = evaluate_model(y_test, y_pred_weighted, y_proba_weighted)\n",
    "print_evaluation_results(metrics_weighted)\n",
    "print(f\"Training Time: {train_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tuning_header",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning mit GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grid_search",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Hyperparameter Tuning (GridSearchCV)\n",
      "======================================================================\n",
      "Parameter Kombinationen: 144\n",
      "Dies kann einige Minuten dauern...\n",
      "\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"Hyperparameter Tuning (GridSearchCV)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Parameter Grid (klein für Raspberry Pi)\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'n_estimators': [100, 200],\n",
    "    'min_child_weight': [1, 3],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "print(f\"Parameter Kombinationen: {np.prod([len(v) for v in param_grid.values()])}\")\n",
    "print(\"Dies kann einige Minuten dauern...\\n\")\n",
    "\n",
    "# GridSearch mit Class Weight\n",
    "xgb_tuned = XGBClassifier(\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False,\n",
    "    scale_pos_weight=scale_pos_weight\n",
    ")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    xgb_tuned,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='f1',  # Fokus auf F1 (Balance von Precision/Recall)\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "search_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ GridSearch abgeschlossen in {search_time/60:.1f} Minuten\")\n",
    "print(f\"\\nBest Parameters:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nBest CV F1-Score: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate_tuned",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Model evaluieren\n",
    "print(\"=\"*70)\n",
    "print(\"TUNED MODEL - EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "best_xgb = grid_search.best_estimator_\n",
    "\n",
    "y_pred_tuned = best_xgb.predict(X_test)\n",
    "y_proba_tuned = best_xgb.predict_proba(X_test)\n",
    "\n",
    "metrics_tuned = evaluate_model(y_test, y_pred_tuned, y_proba_tuned)\n",
    "print_evaluation_results(metrics_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison_header",
   "metadata": {},
   "source": [
    "## 6. Umfassender Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_baseline_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Models aus vorherigem Notebook laden\n",
    "try:\n",
    "    with open(MODELS_DIR / 'best_model_metrics.pkl', 'rb') as f:\n",
    "        metrics_rf_baseline = pickle.load(f)\n",
    "    print(\"✓ Random Forest Baseline Metrics geladen\")\n",
    "except:\n",
    "    metrics_rf_baseline = None\n",
    "    print(\"⚠ Random Forest Baseline nicht gefunden\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison_table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison Table\n",
    "comparison_data = {\n",
    "    'XGBoost Baseline': metrics_baseline,\n",
    "    'XGBoost + SMOTE': metrics_smote,\n",
    "    'XGBoost + Weights': metrics_weighted,\n",
    "    'XGBoost Tuned': metrics_tuned\n",
    "}\n",
    "\n",
    "if metrics_rf_baseline:\n",
    "    comparison_data['RF Baseline'] = metrics_rf_baseline\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data).T\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_df.round(4))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Highlight best per metric\n",
    "print(\"Best per Metric:\")\n",
    "for metric in comparison_df.columns:\n",
    "    best_model = comparison_df[metric].idxmax()\n",
    "    best_value = comparison_df[metric].max()\n",
    "    print(f\"  {metric}: {best_model} ({best_value:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize_comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Overall Metrics Heatmap\n",
    "sns.heatmap(comparison_df.T, annot=True, fmt='.3f', cmap='RdYlGn', \n",
    "            center=0.7, ax=axes[0, 0], cbar_kws={'label': 'Score'})\n",
    "axes[0, 0].set_title('Model Comparison - All Metrics', fontweight='bold')\n",
    "\n",
    "# 2. Key Metrics Bar Plot\n",
    "key_metrics = ['recall', 'precision', 'f1_score', 'roc_auc']\n",
    "comparison_df[key_metrics].plot(kind='bar', ax=axes[0, 1], rot=45)\n",
    "axes[0, 1].set_title('Key Metrics Comparison', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Score')\n",
    "axes[0, 1].set_ylim(0, 1)\n",
    "axes[0, 1].legend(loc='lower right')\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Recall Improvement\n",
    "recall_values = comparison_df['recall'].sort_values()\n",
    "colors = ['red' if v < 0.5 else 'orange' if v < 0.6 else 'green' for v in recall_values]\n",
    "axes[1, 0].barh(range(len(recall_values)), recall_values, color=colors)\n",
    "axes[1, 0].set_yticks(range(len(recall_values)))\n",
    "axes[1, 0].set_yticklabels(recall_values.index)\n",
    "axes[1, 0].set_xlabel('Recall Score')\n",
    "axes[1, 0].set_title('Recall Comparison (Target: > 0.6)', fontweight='bold')\n",
    "axes[1, 0].axvline(x=0.6, color='blue', linestyle='--', label='Target (60%)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 4. Precision-Recall Tradeoff\n",
    "axes[1, 1].scatter(comparison_df['recall'], comparison_df['precision'], \n",
    "                   s=200, alpha=0.6, c=range(len(comparison_df)))\n",
    "for idx, model in enumerate(comparison_df.index):\n",
    "    axes[1, 1].annotate(model, \n",
    "                       (comparison_df.loc[model, 'recall'], \n",
    "                        comparison_df.loc[model, 'precision']),\n",
    "                       fontsize=9, ha='right')\n",
    "axes[1, 1].set_xlabel('Recall')\n",
    "axes[1, 1].set_ylabel('Precision')\n",
    "axes[1, 1].set_title('Precision-Recall Tradeoff', fontweight='bold')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "axes[1, 1].set_xlim(0.3, 1)\n",
    "axes[1, 1].set_ylim(0.3, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_importance_header",
   "metadata": {},
   "source": [
    "## 7. Feature Importance (Best Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature_importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance vom besten XGBoost Model\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': best_xgb.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(15)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(range(len(importance_df)), importance_df['importance'])\n",
    "plt.yticks(range(len(importance_df)), importance_df['feature'])\n",
    "plt.xlabel('Feature Importance (Gain)')\n",
    "plt.title('XGBoost - Top 15 Most Important Features', fontweight='bold', fontsize=14)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Features:\")\n",
    "print(importance_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_model_header",
   "metadata": {},
   "source": [
    "## 8. Best Model speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_best_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bestes Modell auswählen (basierend auf F1-Score und Recall)\n",
    "# Gewichtung: F1 wichtiger, aber Recall muss > 0.55 sein\n",
    "comparison_df['score'] = comparison_df['f1_score'] * 0.6 + comparison_df['recall'] * 0.4\n",
    "best_model_name = comparison_df['score'].idxmax()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL BEST MODEL SELECTION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"\\nMetrics:\")\n",
    "for metric in ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']:\n",
    "    print(f\"  {metric}: {comparison_df.loc[best_model_name, metric]:.4f}\")\n",
    "\n",
    "# Model speichern\n",
    "model_path = MODELS_DIR / 'best_xgboost_model.pkl'\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(best_xgb, f)\n",
    "print(f\"\\n✓ Model gespeichert: {model_path}\")\n",
    "\n",
    "# Metrics speichern\n",
    "metrics_path = MODELS_DIR / 'best_xgboost_metrics.pkl'\n",
    "with open(metrics_path, 'wb') as f:\n",
    "    pickle.dump(metrics_tuned, f)\n",
    "print(f\"✓ Metrics gespeichert: {metrics_path}\")\n",
    "\n",
    "# Feature names speichern\n",
    "feature_path = MODELS_DIR / 'xgboost_feature_names.pkl'\n",
    "with open(feature_path, 'wb') as f:\n",
    "    pickle.dump(list(X.columns), f)\n",
    "print(f\"✓ Features gespeichert: {feature_path}\")\n",
    "\n",
    "# Comparison DataFrame speichern\n",
    "comparison_path = MODELS_DIR / 'model_comparison.csv'\n",
    "comparison_df.to_csv(comparison_path)\n",
    "print(f\"✓ Model Comparison gespeichert: {comparison_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_header",
   "metadata": {},
   "source": [
    "## Zusammenfassung & Erkenntnisse\n",
    "\n",
    "### Was wir erreicht haben:\n",
    "\n",
    "1. **XGBoost Baseline** - Erste XGBoost Implementation\n",
    "2. **SMOTE** - Oversampling der Minority Class\n",
    "3. **Class Weights** - Balance ohne Oversampling\n",
    "4. **Hyperparameter Tuning** - GridSearch für optimale Parameter\n",
    "\n",
    "### Wichtige Erkenntnisse:\n",
    "\n",
    "- **Class Imbalance:** Großer Impact auf Recall\n",
    "- **SMOTE vs Weights:** [Vergleich aus Ergebnissen]\n",
    "- **Hyperparameter:** Können Performance deutlich verbessern\n",
    "- **Recall Improvement:** Von ~45% (Baseline RF) auf ~[X]% (Best XGBoost)\n",
    "\n",
    "### Top Features:\n",
    "- [Werden aus Feature Importance ermittelt]\n",
    "\n",
    "### Nächste Schritte:\n",
    "\n",
    "1. **Weitere Optimierung:**\n",
    "   - Mehr Hyperparameter testen\n",
    "   - Learning Rate Schedule\n",
    "   - Early Stopping\n",
    "\n",
    "2. **Ensemble Methods:**\n",
    "   - Stacking (RF + XGBoost + LR)\n",
    "   - Voting Classifier\n",
    "\n",
    "3. **Production Ready:**\n",
    "   - Model Inference Script\n",
    "   - API Endpoint\n",
    "   - Monitoring & Retraining Pipeline\n",
    "\n",
    "4. **Business Impact:**\n",
    "   - Cost-Benefit Analyse\n",
    "   - Threshold Tuning für optimale Business Metrics\n",
    "   - A/B Testing Framework"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
